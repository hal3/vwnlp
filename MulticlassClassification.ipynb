{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification\n",
    "\n",
    "As convenient as it would be, most things in the world are not binary prediction problems. Many are multiclass prediction problems: given an input, map it to one of 10 classes, or 20 classes, or 100k classes.\n",
    "\n",
    "# 20 Newsgroups\n",
    "\n",
    "As a running example, we'll use the 20 Newsgroups dataset. This contains about 20,000 postings to 20 different newsgroups on usenet; the classification task is to put a posting into the proper newsgroup. The set of newsgroups are:\n",
    "\n",
    "    comp.graphics       comp.os.ms-windows.misc  comp.sys.ibm.pc.hardware  comp.sys.mac.hardware  comp.windows.x\n",
    "    talk.politics.misc  talk.politics.guns       talk.politics.mideast     talk.religion.misc\n",
    "    rec.autos           rec.motorcycles          rec.sport.baseball        rec.sport.hockey\n",
    "    sci.crypt           sci.electronics          sci.med                   sci.space\n",
    "    alt.atheism         soc.religion.christian\n",
    "    misc.forsale\n",
    "\n",
    "which we first need to download and extract. Thankfully, [Jason Rennie has a nice distribution](http://qwone.com/~jason/20Newsgroups/); this is a 14mb download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -rf data/20news-*\n",
    "!curl -o data/20news-bydate.tar.gz http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz\n",
    "!tar zxC data -f data/20news-bydate.tar.gz\n",
    "!echo \"\"\n",
    "!echo \"List of directories:\"\n",
    "!echo \"\"\n",
    "!ls data/20news-bydate-train/\n",
    "!echo \"\"\n",
    "!echo \"First training document in comp.windows.x\"\n",
    "!echo \"\"\n",
    "!head -n40 data/20news-bydate-train/comp.windows.x/64830"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you're probably used to, our first step is to get this into `vw` format.\n",
    "\n",
    "Although `vw` does support labels with names, we're going to map the labels to numbers. We'll come back to named labels later.\n",
    "\n",
    "For features, we'll extract the different email headers to different namespaces, and then the body of the message to a separate namespace. We'll do very stupid tokenization; I'm sure you could do better if you wanted.\n",
    "\n",
    "Here's some code to generate the relevant `vw` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, os\n",
    "from __future__ import print_function\n",
    "import io\n",
    "\n",
    "def sanify(s): return s.replace(':',';').replace('|','/')   # replace : with ; and | with /\n",
    "def tokenize(s): return re.sub('([^A-Za-z0-9 ]+)', ' \\\\1 ', s).split()  # add space around anything not alphanum\n",
    "\n",
    "headerToNamespace = {'Subject': 'S',\n",
    "                     'From': 'F',\n",
    "                     'Organization': 'O',\n",
    "                     'Distribution': 'D',\n",
    "                     'Reply-To': 'R',\n",
    "                     'Keywords': 'K',\n",
    "                     'Summary': 'U' }\n",
    "\n",
    "def readSinglePost(filename):\n",
    "    with io.open(filename, 'r', encoding='iso-8859-1') as h:\n",
    "        namespaces = {}\n",
    "        text = []\n",
    "        inText = False\n",
    "        for l in h.readlines():\n",
    "            words = tokenize(l.strip())\n",
    "            if not inText and len(words) == 0:\n",
    "                inText = True\n",
    "            elif inText:\n",
    "                text += words\n",
    "            elif len(words) > 2 and words[1] == ':' and words[0] in headerToNamespace:\n",
    "                ns = headerToNamespace[words[0]]\n",
    "                namespaces[ns] = words[2:]\n",
    "        namespaces['w'] = text\n",
    "        return namespaces\n",
    "\n",
    "def postToVW(label, namespaces):\n",
    "    ex = str(label)\n",
    "    for ns,words in namespaces.items():\n",
    "        ex += ' |%s %s' % (ns, ' '.join(map(sanify,words[:5000])))  # only take the first 5k words of a post\n",
    "    return ex\n",
    "\n",
    "def readPostsInDirectory(directory):\n",
    "    return [readSinglePost(directory + os.sep + f)\n",
    "            for f in os.listdir(directory)\n",
    "            if  f.isdigit()]   # all file names are just numbers\n",
    "\n",
    "def readAllPosts(baseDirectory, newsgroups):\n",
    "    examples = []\n",
    "    for name,label in newsgroups.items():\n",
    "        posts = readPostsInDirectory(baseDirectory + os.sep + name)\n",
    "        examples += [postToVW(label, post) for post in posts]\n",
    "    return examples\n",
    "\n",
    "def writeFile(filename, examples):\n",
    "    with io.open(filename,'w', encoding='iso-8859-1') as h:\n",
    "        for ex in examples:\n",
    "            print(\"{0}\".format(ex), file=h)\n",
    "\n",
    "newsgroupNames = 'comp.graphics comp.os.ms-windows.misc comp.sys.ibm.pc.hardware comp.sys.mac.hardware comp.windows.x talk.politics.misc talk.politics.guns talk.politics.mideast talk.religion.misc rec.autos rec.motorcycles rec.sport.baseball rec.sport.hockey sci.crypt sci.electronics sci.med sci.space alt.atheism soc.religion.christian misc.forsale'.split()\n",
    "newsgroups = { name: k+1 for k,name in enumerate(newsgroupNames) }   # label ids have to start at 1, not 0\n",
    "\n",
    "train = readAllPosts('data/20news-bydate-train', newsgroups)\n",
    "test  = readAllPosts('data/20news-bydate-test',  newsgroups)\n",
    "\n",
    "import random\n",
    "random.seed(9876)\n",
    "random.shuffle(train)\n",
    "\n",
    "print('read {0} training examples and {1} test examples'.format(len(train), len(test)))\n",
    "\n",
    "writeFile('data/20ng.tr', train)\n",
    "writeFile('data/20ng.te', test)\n",
    "\n",
    "print('files generated:')\n",
    "!wc data/20ng.tr data/20ng.te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"train\"></a> Training VW using One Against All\n",
    "\n",
    "We now have to train `vw`. Basically the only difference between a multiclass run of `vw` and a binary classification run is that you have to tell `vw` how you want it to solve the multiclass problem, and how many classes there are.\n",
    "\n",
    "A very reasonable method for multiclass classification is **One Against All** (OAA), which, on $K$ classes, trains $K$ regressors, each to predict a particular class. We can do this for 20 Newsgroups by simply saying \"`--oaa 20`\" on the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw -k -c -b 27 --oaa 20 -d data/20ng.tr -f data/20ng.model --passes 20 --holdout_after 10001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, we have a holdout loss of about 9.0% -- this is just the fraction of errors we make. This is pretty reasonable; [Jason Rennie reports 15% error](http://people.csail.mit.edu/jrennie/writing/loocv.pdf) also on the test data. But we expect test performance should be similar to heldout performance. Let's make sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw -t -i data/20ng.model -d data/20ng.te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so 19% error is actually much worse than 9% error. This suggests the training and test distributions don't match up entirely. Oh well. Still ballpark reasonable for putting no effort into it.\n",
    "\n",
    "A few notes:\n",
    "\n",
    "* When I trained the model, I used a slightly larger number of bits than before. This is because in OAA, we're storing 20 different weight vectors in the same amount of space. If you want each weight vector to have the, say, 24 bits of representation, then you should run with 28 or 29 total bits to account for the fact that you need to pack in 20 different vectors.\n",
    "\n",
    "* When I tested the model I didn't need to say `--oaa` again; this is stored in the saved model and therefore unnecessary.\n",
    "\n",
    "# <a id=\"human\"></a> Getting a Human Readable Model\n",
    "\n",
    "The mechanism for getting a human-readable model out is basically the same as always. We train a model, then rerun with `--invert_hash -t` over the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw -t -i data/20ng.model -d data/20ng.tr --invert_hash data/20ng.model.readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this is quite slow and memory-hogging on my laptop (about 1gb ram, a few minutes). On the bright side, this *does* demonstrate that there's a big win for now dealing with strings during normal training!\n",
    "\n",
    "Once that's done, we can look at specific features. For instance, we might be interested in the feature \"DoD\" (probably \"Department of Defense\") in the \"w\" (words) namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!grep '^w^bike\\[' data/20ng.model.readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to read this is as follows. For the `w` namespace, there's a feature `bike`. It has a separate weight for each of the classes `0..19` (this is potentially an annoying source of off-by-one erorrs since in the input file we have classes `1..20`... sigh, sorry!).\n",
    "\n",
    "The class that `bike` is most indicative of is vw label `[10]` (with a weight of 0.267542). This is our class 11. Which class is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k, v in newsgroups.items():\n",
    "    if v == 11:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so it's `rec.motorcycles`. This passes the sanity check!\n",
    "\n",
    "We can also just look at the top weighted features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat data/20ng.model.readable | tail -n+14 | sort -t: -k3g | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top two are the `w`ord namespace, but the third one (`S^Windows`) is the Subject line namespace. In this case it's indicative of label `[1]` which is class 2, which, looking above, is `comp.os-ms-windows.misc`.\n",
    "\n",
    "# <a id=\"raw\"> Getting (Raw) Predictions\n",
    "\n",
    "As before, often we want to get label predictions (which class?) and/or per-label scores (a ranking?). We can get the class predictions with `-p` and the score with `-r`. Let's do this for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw -t -i data/20ng.model -d data/20ng.te -p data/20ng.te.pred -r data/20ng.te.raw --quiet\n",
    "!head data/20ng.te.pred data/20ng.te.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `.pred` file, we can see the actual predictions (20, 20, 20, ..., 3, 20, 20, ...). These are just the predicted labels.\n",
    "\n",
    "In the `.raw` file, we can see per-class scores. This is still one-line-per-example. For the first line, we can see that every class 1..19 has a negative score and 20 has a positive score. Thus 20 \"wins\" and is the prediction.\n",
    "\n",
    "# <a id=\"namd\"></a> Using Named Classes\n",
    "\n",
    "Sometimes it is inconvenient to have to map class ids to numbers. Using \"named classes\" we can avoid this. The downside is that the class names have to, instead, be specified on the command line.\n",
    "\n",
    "<font size=\"-2\">[Note: you might wonder why `vw` can't just read in named classes as it reads the file and do it's own internal counting of numbers. This is because sometimes you want to run multiple instances of `vw` in parallel. When these instances communicate, they need to agree on the numbering scheme.]</font>\n",
    "\n",
    "To use named classes, you simply: (1) replace the label numbers in the input file(s) with strings; and (2) specify the names on the command line. Here's an example for 20 Newsgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "namedNewsgroups = { name:name for name in newsgroupNames }\n",
    "\n",
    "train = readAllPosts('data/20news-bydate-train', namedNewsgroups)\n",
    "test  = readAllPosts('data/20news-bydate-test',  namedNewsgroups)\n",
    "\n",
    "random.seed(9876)\n",
    "random.shuffle(train)\n",
    "\n",
    "print('read {0} training examples and {1} test examples\\n'.format(len(train), len(test)))\n",
    "print('all labels: {0}\\n'.format(','.join(newsgroups)))\n",
    "\n",
    "writeFile('data/20ng-named.tr', train)\n",
    "writeFile('data/20ng-named.te', test)\n",
    "\n",
    "print('files generated:')\n",
    "!wc data/20ng-named.tr data/20ng-named.te\n",
    "print('\\nsome of the examples 9first few words:')\n",
    "!head data/20ng-named.tr | cut -d' ' -f1-10\n",
    "print('\\ntraining vw:')\n",
    "!vw -k -c -b 27 --oaa 20 -d data/20ng-named.tr -f data/20ng-named.model --passes 20 --holdout_after 10001 --named_labels misc.forsale,talk.politics.guns,comp.sys.mac.hardware,talk.politics.misc,soc.religion.christian,comp.graphics,sci.med,talk.religion.misc,comp.windows.x,comp.sys.ibm.pc.hardware,talk.politics.mideast,comp.os.ms-windows.misc,sci.crypt,sci.space,alt.atheism,rec.sport.hockey,rec.sport.baseball,sci.electronics,rec.autos,rec.motorcycles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we might want to get (raw) predictions out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw -t -i data/20ng-named.model -d data/20ng-named.te -p data/20ng-named.te.pred -r data/20ng-named.te.raw --quiet\n",
    "!head data/20ng-named.te.pred data/20ng-named.te.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see that the predictions shows the name of the class. The raw predictions, however, uses the internal numbering. The question then is \"what labels do the different numbers correspond to?\" The answer is that it's just the order you used in the \"`--named_labels`\" argument. In that argument, the first item was `misc.forsale` so that's what `1:...` means here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# <a id=\"scaling\"></a> Scaling Up to More Classes\n",
    "\n",
    "In the previous example, we had only 20 classes. For lots of problems, we have more. The problem we'll consider now is the Quiz Bowl question answering task. In this problem, we get a long-form question and have to provide an answer. In the full version of the problem, the goal is to answer the question as *early* as possible (i.e., before the entire question is read). For simplicity here, we'll focus on the simpler problem of just predicting the answer at the end of the question. Our [QANTA system](), which [recently bested Ken Jennings](), get an error rate of about 20% at the end of the question on this task (though trained on more data and also with extra Wikipedia resources). [Mohit Iyyer]() has made [the non-proprietary question/answer pairs available for download]().\n",
    "\n",
    "As usual, we begin by grabbing the data (this is a 31mb download):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -rf data/question-* data/quizbowl*\n",
    "!curl -o data/question_data.tar.gz http://cs.umd.edu/~miyyer/data/question_data.tar.gz\n",
    "!tar zxC data -f data/question_data.tar.gz\n",
    "!echo \"\"\n",
    "!echo \"counting the number of questions:\"\n",
    "!wc -l data/question_data/questions.csv\n",
    "!echo \"\"\n",
    "!echo \"looking at the first question:\"\n",
    "!echo \"\"\n",
    "!head -n2 data/question_data/questions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In this format, \"`|||`\" means \"sentence boundary\".)\n",
    "\n",
    "And now we have to read it in. In this case it's particularly easy because it's generated with Python's csvwriter. The data is already split into train/dev/test for us too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "\n",
    "def readQuizBowlData(filename):\n",
    "    train,dev,test = [],[],[]\n",
    "    data = csv.reader(io.open(filename, 'r', encoding='iso-8859-1').readlines())\n",
    "    header = next(data, None)\n",
    "    if set(header) != set(['Question ID', 'Fold', 'Category', 'Answer', 'Text']):\n",
    "        raise Exception('data improperly formatted')\n",
    "    for item in iter(data):\n",
    "        y = item[3]        \n",
    "        x = sanify(' '.join(tokenize(item[4].replace('|||',''))))\n",
    "        if   item[1] == 'train': train.append( (x,y) )\n",
    "        elif item[1] == 'dev'  :   dev.append( (x,y) )\n",
    "        elif item[1] == 'test' :  test.append( (x,y) )\n",
    "    return train,dev,test\n",
    "\n",
    "def makeLabelIDs(train, outputFile):\n",
    "    labelIds = { label: k+1 for k,label in enumerate(set([y for x,y in train])) }\n",
    "    labelIds['***UNKNOWN***'] = len(labelIds)+1\n",
    "    with io.open(outputFile, 'w') as h:\n",
    "        for label,k in labelIds.items():\n",
    "            print(\"{0}\\t{1}\".format(k, label), file=h)\n",
    "    return labelIds\n",
    "\n",
    "def writeVWFile(filename, data, labelIds):\n",
    "    unknownId = labelIds['***UNKNOWN***']\n",
    "    with io.open(filename,'w') as h:\n",
    "        for x,y in data:\n",
    "            print(\"{0} | q {1}\".format(labelIds.get(y, unknownId), x), file=h)\n",
    "            \n",
    "train,dev,test = readQuizBowlData('data/question_data/questions.csv')\n",
    "traindev = train + dev\n",
    "random.seed(9876)\n",
    "random.shuffle(traindev)\n",
    "labelIds = makeLabelIDs(train, 'data/quizbowl.labels')\n",
    "writeVWFile('data/quizbowl.trde', traindev, labelIds)\n",
    "writeVWFile('data/quizbowl.te',   test    , labelIds)\n",
    "print('maximum label id = {0}'.format(len(labelIds)+1))\n",
    "!wc data/quizbowl.t[re]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 2322 labels and 17805 train/dev examples. This is obviously going to be a pretty hard problem without auxiliary information. We can still make a go of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw -k -c -b 27 -d data/quizbowl.trde --oaa 2322 --passes 20 -f data/quizbowl.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's 27% holdout error, which, all told, is not that bad. Of course the real question is how well we do on test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw -d data/quizbowl.te -t -i data/quizbowl.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, 20% test loss. This is actually quite impressive for basically doing no work. One legitimate question is what is the frequency of the most frequent class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "labelFreq = Counter([y for x,y in train])\n",
    "print(labelFreq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So out of 8800 training examples, the most frequent one only occurs twenty times, which is not very much. Of course the \"unknown\" label could dominate in test. It's unlikely we would predict that label since we never saw it at training time, but it's possible. Let's make sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat data/quizbowl.te | cut -d' ' -f1 | sort | uniq -c | sort -g | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the most frequent label *is* indeed the \"never seen this answer before\" label, but it still only appears in 75/2600 examples, which is only about 2.8%. Definitely not a winning strategy!\n",
    "\n",
    "# <a id=\"alter\"></a> Alternatives to One Against All\n",
    "\n",
    "You may have noticed that training on the quizbowl data, especially for a relatively small data set, was kind of slow. This is because OAA time complexity scales like O(K), where K is the number of classes. This is fine for something like 20 Newsgroups, but bad for problems with lots of classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!time vw -k -c -b 29 -d data/quizbowl.trde --log_multi 2322 --passes 10 --early_terminate 999 -f data/quizbowl.model2\n",
    "!vw -d data/quizbowl.te -t -i data/quizbowl.model2 2>&1 | grep '^average loss'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that took (on my laptop) about 2 minutes of wallclock time and gets an error rate of 40%, which is significantly worse than OAA. If we give OAA approximately the same amount of time, this is what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!time vw -k -c -b 27 -d data/quizbowl.trde --oaa 2322 --passes 2 -f data/quizbowl.model\n",
    "!vw -d data/quizbowl.te -t -i data/quizbowl.model2 2>&1 | grep '^average loss'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that took about the same amount of time and got a loss of 40%.\n",
    "\n",
    "In this case, it appears there is a flat out tie :(. However, as the number of classes grows, eventually OAA becomes untenable and you really cannot afford the O(K) computation. More details are in the [LOMTree paper](http://arxiv.org/abs/1406.1822)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
