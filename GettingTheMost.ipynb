{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Most Out Of VW\n",
    "\n",
    "`vw` has *lots* of command-line arguments. For some of them you have to learn a little bit about how `vw` works internally. This notebook assume that you've made your way successfully through the [Getting Started](GettingStarted.ipynb) notebook first. In this notebook we'll make our way through the following topics:\n",
    "\n",
    "* [Adjusting the number of bits used to store models](#capacity)\n",
    "* [Using some NLP-style feature extractors](#capacity) ([word affixes](#affixes), [spelling](#spelling) and [n-grams](#ngram))\n",
    "* [Changing the loss function that's being optimized](#loss) and [probabilistic output](#prob)\n",
    "* [Getting a human-readable model out of `vw`](#human)\n",
    "* [Changing the default holdout settings (eg to use NLP-style \"development data\")](#holdout)\n",
    "* [Namespaces](#ns) and [quadratic features](#quad)\n",
    "* [Regularization](#reg)\n",
    "* [Neural networks](#nn)\n",
    "* [Summary](#summary)\n",
    "\n",
    "# <a id='capacity'></a> Increasing Representational Capacity (and memory usage)\n",
    "\n",
    "Let's start with our previous training example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, in order to be fast, `vw` never stores any strings. When it reads your input (in which your features were represented as strings!), it *immediately* hashes the strings to some feature index. By default, it uses $2^{18}$ possible feature indices; this magic number $18$ is the \"number of bits\" used to store the weights in the model. (This is something of a misnomer: it's really the number of parameters in the learning algorithm, which is roughly the number of floats.)\n",
    "\n",
    "What does this hashing accomplish? Well, it accomplishes speed because no string manipulation ever happens. However, it comes with two downsides. The first is that you can get hash collisions. In particular, you might have to different features (i.e., different strings) that hash to the same location. From the learning algorithm's perspective, this means these two features are indistinguishable.\n",
    "\n",
    "Remember that hash collisions are incredibly common. In NLP land, we often have several hundred thousand unique features. By a simple birthday-paradox type argument, we know that the probability of collision when you have $k$ items into $N$ buckets is approximately $1-\\exp\\left[\\frac {k(k-1)} {2N}\\right]$. In this case, with $N=2^{18}$, even with only $2000$ unique features, the probability of collision is already 99.95%. With $100k$ features it's basically guaranteed.\n",
    "\n",
    "The solution is to increase the number of bits used in the representation. This will (a) reduce the number of collisions, (b) make `vw` take more RAM, (c) make `vw` somewhat slower, and (d) make the resulting models larger on disk. Currently, the maximum number of bits that `vw` will let you use is 31. I don't suggest using this; it means `vw` will consume about 8GB of memory while running and the resulting file may take as much as 2GB of disk space. [Runtime memory will be 4 times larger than disk space because the optimization algorithms need extra working memory.] But note that with 100k unique features, even with 31 bits, the probability of collision is 99.1%. It will happen.\n",
    "\n",
    "In practice I usually use around 24 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, increasing the number of bits did not help test accuracy, but we will see that, when we add additional features, it becomes more important.\n",
    "\n",
    "# <a id='nlp'></a>  Fun NLP-esque Features for Free\n",
    "\n",
    "One nice thing about `vw` is that it internally supports \"extra feature\" generation. The main useful features are: word prefixes and suffixes, spelling features, and ngram features.\n",
    "\n",
    "## <a id='affixes'></a> Word Affixes\n",
    "\n",
    "For NLP tasks that mostly depend on the *meaning* (aka \"semantics\") of words, we often don't care about the funny little things that come at the ends of words. For instance, for sentiment classification, the words `awesome` and `awesomeness` are likely to roughly mean the same thing. For other tasks, like part of speech tagging, it's the suffixes that might matter most: words that end in `-ness` are much more likely to be adjectives than anything else.\n",
    "\n",
    "`vw` can automatically generate word prefixes and suffixes for you, using the `--affix` feature. For instance, if you add \"`--affix +5,-3`\" to the command line, this says to automatically compute (and add as new features) 5-character prefixes (that's the `+`) and three character suffixes (that's the `-`).\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24 --affix +6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was (somewhat) helpful -- the holdout loss dropped from 14.4% to 13.8%. Perhaps not an ACL paper, but at least it's doing something! Note, satisfyingly, that the number of features approximately doubled. (In fact, when the old feature count was 955, the new feature count is 1909. This is because $1909=1 + 954 * 2$. The 955 old features includes 954 words and a bias feature. Each word gets an affix so we have $954 * 2 = 1908$ real features, plus a bias.)\n",
    "\n",
    "## <a id='spelling'></a> Spelling Features\n",
    "\n",
    "Spelling features are *super* useful for tasks where things like capitalization, years, numbers, etc. matter a lot. In other words, tasks *not at all* like sentiment classification.\n",
    "\n",
    "In `vw`, the spelling features option tells it to generate new features based on the word forms seen. For example, a word \"Alice\" has the word form \"Aaaaa\" (meaning: a capital letter followed by four lowercase letters); \"VanBuren\" has the form \"AaaAaaaa\". The general rule is that digits 0-9 get mapped to \"0\", letters a-z to \"a\", letters A-Z to \"A\", period to \".\" and anything else to \"#\". Thus, \"xY9s,3.80vaq\" gets mapped to \"aA0a#0.00aaa\" and this new \"word form\" is used as a new feature.\n",
    "\n",
    "To turn on spelling features, you simply add `--spelling _` to the command line. We can do this with little expectation that it will help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24 --spelling _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, indeed it did not help and it actually hurt slightly (14.4% error to 15.0% error).\n",
    "\n",
    "You might wonder what the \"`_`\" in the command line means; for now, don't worry about it. We'll come back to this when we talk about namespaces.\n",
    "\n",
    "## <a id='ngram'></a> N-gram Features\n",
    "\n",
    "Our current representation for learning is bag of words. Some times looking at a single word at a time is insufficient and we want to, instead, look at (contiguous) sequences of words: ngrams.\n",
    "\n",
    "Given an example text \"the monster ate a sandwich\", if we were to augment this with bigram features, we would get \"the_monster monster_ate ate_a a_sandwich\". In cases where word bigrams are useful, this is going to be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24 --ngram 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that was super useful! Loss dropped from 14.4% to 11.9%! We can try trigram features too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24 --ngram 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that didn't help any more.\n",
    "\n",
    "We can, however, now see that the number of bits matters. If we drop the number of bits back down to 18, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 18 --ngram 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is no better than we started with.\n",
    "\n",
    "More specifically: **if we hadn't increased the number of bits, we would have concluded that ngram features weren't useful!** This is why I always use as many bits as I can tolerate (sometimes up to 27 or 29).\n",
    "\n",
    "Finally, `vw` can do \"skip ngrams\" too. This means that instead of only looking at bigrams of adjacent words, you can look at bigrams with some gap. For instance, if you say `--ngram 2 --skips 1`, this means \"compute all bigrams that have at most one gap in them.\" For our favorite sentence \"the monster ate a sandwich\", you would get the default bigram features (\"the_monster monster_ate ate_a a_sandwich\") and *also* the skips (\"the_ate monster_a ate_sandwich\").\n",
    "\n",
    "Note: as you increase to, say, four-grams, this automatically includes bigrams and trigrams. As you increase number of skips, you get all the lower order skips too.\n",
    "\n",
    "# <a id='loss'></a> Changing the Loss Function\n",
    "\n",
    "By default, `vw` optimizes squared loss. This means that if the correct label for an example is +1, and the model predicts -1, the error is 4.0. However, if the model predicts +3, the error is still 4.0, even though it's making the right binary prediction. Squared loss has the nice property that it estimates means. But it's not necessarily the most natural loss for classification problems.\n",
    "\n",
    "Many people prefer logistic loss (which gives a nice probabilistic interpretation) or hinge loss (which, when combined with regularization, yields support vector machines). You can switch the loss function with a simple command-line flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model --loss_function logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prob'></a> If you're using logistic loss, you can get probabilistic predictions out of you model by using `-r`(aw) output; for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary -i data/sentiment.model -t -r data/sentiment.te.pred data/sentiment.te --quiet\n",
    "!head data/sentiment.te.pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These predictions are values *before* being hit with a logistic function. To get probabilities, map $z \\mapsto \\frac 1 {1 + \\exp(-z)}$, as in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head data/sentiment.te.pred | perl -ne '$a = 1/(1+exp(-$_)); print \"$a\\n\";'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can alternatively switch to hinge loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model --loss_function hinge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, logistic loss does awesome (dropping the error from 14.4% to 11.3%) and hinge does crummy (increasing to 15.6%). This is perhaps because with hinge we probably want to regularize, which we'll get back to later.\n",
    "\n",
    "# <a id='human'></a> Getting a Human Readable Model\n",
    "\n",
    "Admittedly one of the most annoying things with `vw` is getting a human-readable model out. In some more complex cases, this is nearly impossible. And even in simple cases (like those here), it's cumbersome. This is because of the fact that `vw` doesn't store strings. So if you want to get a mapping from features-to-weights out of `vw` you have to jump through some hoops.\n",
    "\n",
    "What are those hoops?\n",
    "\n",
    "First you have to learn a model and save it to disk. Fine, we know how to do that. You then have to instruct `vw` to load the saved model and take another pass over your training data, and save the results to disk. It has to take another pass over the data because as it makes that final pass, it actually *does* store the strings in memory (if you tell it to) so it can generate the human readable file.\n",
    "\n",
    "What does this look like in practice? First, we generate a model. For a warm-up, we won't use any fancy features. Then, we have to do a second incantation of `vw` and tell it where to store the resulting human-readable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model --loss_function logistic --quiet\n",
    "!vw -i data/sentiment.model -t --invert_hash data/sentiment.model.readable data/sentiment.tr --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first command here trains the model as before. The second says: start from that pre-trained model; go into test mode (so that you don't adjust any of the weights of the model); store the resulting readable model (`--invert_hash`) into the specified file; and read from `data/sentiment.tr` (you have to re-read from the same training data).\n",
    "\n",
    "We can now look at `data/sentiment.model.readable` to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -n40 data/sentiment.model.readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beginning of this output is some header information that tells you a bit about the type of model that was stored. After the `options:` line there's the `:0` line, and then after that you get a list of `feature:hash:weight` triples. For instance, the feature \"`earth`\" was hashed to position 20130, and has a feature weight -0.056972, which means it's (very) mildly indicative of the negative class. These words are actually sorted; the reason \"earth\" pops up at the top is because the \"e\" in the string is some weird unicode \"e\" and not the normal ascii \"e\".\n",
    "\n",
    "We can extract just the features by dropping off the first 12 lines of the output, and then sort by the feature weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat data/sentiment.model.readable  | tail -n+13 | sort -t: -k3nr | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Ignore the broken pipe issues, which are a ipython notebook joy.)\n",
    "\n",
    "Here, we've dropped the first 12 lines, then sorted the remaining by the third column in reverse numerical order (`-k3nr`) where columns are separated by colons (`-t:`) and then looked at the top 10.\n",
    "\n",
    "These are kind of weird features to see at the top of a sentiment classification data set. Presumably this means that people like Hamlet and probably also like Matt Damon. But there's very little more we can get from this.\n",
    "\n",
    "We can look at the most negative features too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat data/sentiment.model.readable  | tail -n+13 | sort -t: -k3nr | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are much more reasonable, and the magnitudes are significantly larger (if negative), suggesting that these are probably what is really being used to make good predictions.\n",
    "\n",
    "We can do the same thing with the model with ngram features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model --loss_function logistic --ngram 3 -b 24 --quiet\n",
    "!vw -i data/sentiment.model -t --invert_hash data/sentiment.model.readable data/sentiment.tr --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice this took a bit longer to run because there are *lots* of ngrams. We can see the output as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!echo \"Top positive features\"\n",
    "!cat data/sentiment.model.readable  | tail -n+13 | sort -t: -k3nr | head\n",
    "!echo \"\"\n",
    "!echo \"Top negative features\"\n",
    "!cat data/sentiment.model.readable  | tail -n+13 | sort -t: -k3nr | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all of these, there's only one ngram feature that pops up as negative, \"`puppetry^and/or^hate`\". You can google this trigram if you really want to know what it's from. Amusingly, \"attempt\" also comes up as a negative word. I saw this once in a review for one of my papers: \"this paper attempts to do XYZ.\" Not a good sign.\n",
    "\n",
    "# <a id='holdout'></a> Changing `vw`'s Default Holdout Settings\n",
    "\n",
    "If you recall from the introduction, the default way `vw` works for doing multiple passes is: on the first pass, perform progressive validation; on subsequent passes, use every 10th example as a heldout \"validation\" example. And to stop optimizing when things don't improve for three passes.\n",
    "\n",
    "These are reasonable defaults, but somewhat at odds with the behavior I often want.\n",
    "\n",
    "First, I often *don't* want `vw` to do early stopping. If I tell it to do 20 passes, then by golly it should do 20 passes. This is easy. I just say `--early_terminate 999`. This means that instead of needing 3 passes of no-improvement in order to terminate, it now needs 999. Since I never run that many passes, this is a good default to say \"don't stop early.\" However, if *will* still output only the best model found.\n",
    "\n",
    "More relevant, often in NLP we have training data, development data, and test data. And I want to get validation performance on the development data rather than every-10th-example. You can accomplish this with `--holdout_after N`. What this means is: instead of doing every-10th-example as validation, use the first (N-1) examples as training data, and anything after that as development data.\n",
    "\n",
    "Putting these together, I usually do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model --early_terminate 999 --holdout_after 1401"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the important thing is that the first 1400 examples are used as training data, the the remaining examples (in this case, 200) are used as heldout data. The average loss reported is then *precisely* the average loss on this heldout data.\n",
    "\n",
    "# <a id='ns'></a>  Namespaces and quadratic features\n",
    "\n",
    "For this part of the tutorial to make sense, we have to make our task a little more interesting.\n",
    "\n",
    "Many people who do sentiment analysis start from a *sentiment lexicon*: basically, a list of positive-ish and negative-ish words. There are [lots of sentiment lexicons](http://sentiment.christopherpotts.net/lexicons.html). We will use [the one from Bing Liu](http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html). First, let's download it and decompress it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   404  100   404    0     0    333      0  0:00:01  0:00:01 --:--:--   333\n",
      "\n",
      "UNRAR 5.31 freeware      Copyright (c) 1993-2016 Alexander Roshal\n",
      "\n",
      "data/opinion-lexicon-English.rar is not RAR archive\n",
      "No files to extract\n",
      "ls: data/*words.txt: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -f data/*words.txt\n",
    "!curl -o data/opinion-lexicon-English.rar https://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar\n",
    "!rar x data/opinion-lexicon-English.rar  data\n",
    "!ls -l data/*words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at some of the positive and negative words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -n50 data/*-words.txt | grep -v '^;'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this, I dropped lines that begin with \"`;`\" because these are comments in the files.\n",
    "\n",
    "We now want to go back and generate some new data files for `vw` that include lexicon features. In particular, we will include *both* the bag of words representation *as well as* lexicon features. The lexicon features we will use are very simple: the log of the count of words in the document that are on the positive list, and the log of the count on the negative list. We use logs because getting more positive words has diminishing returns.\n",
    "\n",
    "To do this, we'll write a bit more python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "def loadLexicon(filename):\n",
    "    with io.open(filename, 'r', encoding='iso-8859-1') as h:\n",
    "        return set(l.strip() \n",
    "                    for l in h.readlines()\n",
    "                    if  not l.startswith(';') and len(l) > 1)\n",
    "\n",
    "import math\n",
    "def countLexiconWords(text, lexicon):\n",
    "    return math.log(1.0 + len([w for w in text if w in lexicon]))\n",
    "\n",
    "positiveLexicon = loadLexicon('data/positive-words.txt')\n",
    "negativeLexicon = loadLexicon('data/negative-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a copy of the two lexicons and we want to generate `vw` examples.\n",
    "\n",
    "But we have two different types of features. We have the original bag of words features. And we have these lexicon features. We'd like to keep them separate.\n",
    "\n",
    "This is where feature namespaces come in. We're going to create examples with *two* namespaces, one for the bag of words (let's call it the `w` namespace) and one for the lexicon features (let's call that the `l` namespace). In `vw`, namespaces are separated by pipes, so an example might look like:\n",
    "\n",
    "    +1 |l pos:5 neg:2 |w some words might go here ...\n",
    "    \n",
    "In addition to having two namespaces, this example also shows how to use feature values. By default, all features in a `vw` example get a value of one. If you want to override this, you can say something like \"`pos:5`\", which means that there's a single feature (called \"`pos`\") that has a feature value of 5.\n",
    "\n",
    "Let's generate data like this. Some of the code is copied from the Getting Started tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def textToVW(lines):\n",
    "    return ' '.join([l.strip() for l in lines]).replace(':','COLON').replace('|','PIPE')\n",
    "\n",
    "def fileToVW(inputFile, posLex, negLex):\n",
    "    text     = textToVW(open(inputFile,'r').readlines())\n",
    "    words    = text.split()\n",
    "    posCount = countLexiconWords(words, posLex)\n",
    "    negCount = countLexiconWords(words, negLex)\n",
    "    return '|l pos:%g neg:%g |w ' % (posCount,negCount) + text\n",
    "\n",
    "import os\n",
    "def readTextFilesInDirectory(directory):\n",
    "    return [fileToVW(directory + os.sep + f, positiveLexicon, negativeLexicon) \n",
    "            for f in os.listdir(directory)\n",
    "            if  f.endswith('.txt')]\n",
    "\n",
    "examples = ['+1 ' + s for s in readTextFilesInDirectory('data/txt_sentoken/pos')] + \\\n",
    "           ['-1 ' + s for s in readTextFilesInDirectory('data/txt_sentoken/neg')]\n",
    "\n",
    "print('{0} total examples read'.format(len(examples)))\n",
    "print('first example: {0}...'.format(str(examples[ 0][:70])))\n",
    "print('last  example: {0}...'.format(str(examples[-1][:70])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least based on these two examples, this seems promising: the positive/negative lexicon features seem to correlate with the labels!\n",
    "\n",
    "Let's generate a new `vw` training file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from __future__ import print_function\n",
    "\n",
    "random.seed(1234)\n",
    "random.shuffle(examples)   # this does in-place shuffling\n",
    "\n",
    "\n",
    "def writeToVWFile(filename, examples):\n",
    "    with open(filename, 'w') as h:\n",
    "        for ex in examples:\n",
    "            print(\"{0}\".format(ex), file=h)\n",
    "            \n",
    "writeToVWFile('data/sentiment-lex.tr', examples[:1600])\n",
    "writeToVWFile('data/sentiment-lex.te', examples[1600:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're in a position where we can train a model. Let's use exactly the same command as earlier, which got us 11.9% error rate, but using the new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, disappointingly, we still get 11.9% error rate!\n",
    "\n",
    "One thing we can do that is useful is *turn off* a subset of the namespaces. For instance, if we want to *only* use the lexicon features, we can tell `vw` to ignore the `w` namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 --ignore w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall that's not too impressive: 31.9% error. On the other hand, this is just using two features (plus a bias).\n",
    "\n",
    "## <a id='quad'></a> Quadratic features\n",
    "\n",
    "The real magic comes from *feature combination*. For instance, the first example from above looks like:\n",
    "\n",
    "    +1 |l pos:4.54329 neg:3.4012 |w note COLON some may consider portions ...\n",
    "    \n",
    "There might be reason to believe that looking at *pairs* of features between the `l` and `w` namespaces would be useful. In this case, these features would be things like:\n",
    "\n",
    "    note_pos:4.5 note_neg:3.4 COLON_pos:4.5 COLON_neg:3.4 some_pos:4.5 ...\n",
    "    \n",
    "(I've rounded 4.54329 to 4.5 and 3.4012 to 3.4 for brevity.)\n",
    "\n",
    "This allows you to model interactions among features. `vw` will do this automatically for you with `-q` (quadratic) features. For example, you can ask for all pairs of features between these two namespaces as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 -q wl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that, our loss has dropped from 11.9% to 11.3%. Not a huge win, but something. Note that the number of features per example has approximately tripled here.\n",
    "\n",
    "You can go crazy if you want and add quadratic features between the `l` namespace and itself and the `w` namespace and itself too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 -q wl -q ll -q ww"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is significantly slower *and* significantly worse, basically because there are now hundreds of thousands of features, and the model has overfit.\n",
    "\n",
    "Minor note: when creating quadratic features, you can use `:` as a wildcard to refer to \"any namespace\", for instance \"`-q l:`\" pairs `l` with all other namespaces; \"`-q ::`\" pairs all namespaces with all other namespaces.\n",
    "\n",
    "# <a id='reg'></a> Regularization\n",
    "\n",
    "Regularization is a sometime-helpful method for preventing your model from overfitting to the training data. Once you have a reasonable amount of data, I find regularization in `vw` to be relatively **un**helpful, largely because the underlying learning algorithm is quite good. But for small or modest data set sizes, like the sentiment data, it is plausibly useful.\n",
    "\n",
    "`vw` has two built-in forms of regularization: $\\ell_2$ (\"Gaussian\") regularization and $\\ell_1$ (\"sparse\") regularization. You can combing them if you want to get \"elastic net\" regularization. Both forms for regularization require a strength parameter, which usually should be quite small and must be tuned carefully. Doing $\\ell_1$ has the advantage of often producing models with lots of zeros. Here are some runs with both, where I've chosen regularization strengths by hand that work well. We're also going to save a readable model to disk so we can look at how many features are being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 -q wl --l2 0.0001 -f data/sentiment-lex.model\n",
    "!vw -i data/sentiment-lex.model -t --invert_hash data/sentiment-lex.model.readable data/sentiment-lex.tr --quiet\n",
    "!echo \"\"\n",
    "!echo \"total number of features:\"\n",
    "!tail -n+13 data/sentiment-lex.model.readable | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, a regularization strength of 0.0001 has dropped the error rate from 11.3% to 10.0%! And the model is using 130k features.\n",
    "\n",
    "We can also try $\\ell_1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 -q wl --l1 0.000001 -f data/sentiment-lex.model\n",
    "!vw -i data/sentiment-lex.model -t --invert_hash data/sentiment-lex.model.readable data/sentiment-lex.tr --quiet\n",
    "!echo \"\"\n",
    "!echo \"total number of features:\"\n",
    "!tail -n+13 data/sentiment-lex.model.readable | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a slightly higher loss (the original value of 11.9%) but using fewer features: only 97k in this case. We can increase the strength of the regularizer and get fewer features at the cost of higher error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 -q wl --l1 0.00001 -f data/sentiment-lex.model\n",
    "!vw -i data/sentiment-lex.model -t --invert_hash data/sentiment-lex.model.readable data/sentiment-lex.tr --quiet\n",
    "!echo \"\"\n",
    "!echo \"total number of features:\"\n",
    "!tail -n+13 data/sentiment-lex.model.readable | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that if we're willing to suffer a bit more loss (in this case, 15.0%), we can get a model that's about a quarter of the size.\n",
    "\n",
    "If you add both `--l1` and `--l2` (and tune the two corresponding hyperparameters), you get elastic net. I've never had this be particularly effective.\n",
    "\n",
    "# <a id='nn'></a>  Neural networks\n",
    "\n",
    "We've already seen several ways of achieving non-linearity if `vw`: quadratic features, ngrams, etc. A more traditional approach is to add a hidden layer of representation, yielding a two-layer feed-forward neural network. To achieve this, you simply say `--nn 10`, to get ten hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function hinge -b 24 --nn 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: I switched to hinge loss. With logistic loss, you get about 50% error which is horrible. This has to do with scaling of the gradients.) Well, that is disappointing. Our loss went from around 12% to around 16%.\n",
    "\n",
    "One reason is that the original bag of words features are actually quite useful, and by forcing them through a 5-unit hidden layer you lose a lot of information. A solution to this problem is `--inpass` which adds additional edges to the neural network directly from the input to the output. So you basically get the best of both worlds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function hinge -b 24 --nn 10 --inpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so this is better, we're down to 13.8% error. But what else can we do? **DROPOUT**! This is a particularly useful method of regularization for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function hinge -b 24 --nn 10 --inpass --dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, disappointed once again: loss is back up to 14.4%.\n",
    "\n",
    "By playing around with: (a) loss function, (b) size of the hidden units and (c) dropout, I **was** able to get a model that's slightly better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 --nn 10 --inpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, here, that we've switched *back* to logistic loss. When you use `--inpass` and get features directly from the input, you don't have the same problems with logistic loss that you do when you don't use `--inpass`. It definitely requires some fiddling.\n",
    "\n",
    "# <a id='summary'></a>Summary\n",
    "\n",
    "In this notebook, we've learned lots of ways to get extra features from `vw`. Here's a brief summary:\n",
    "\n",
    "* Using `-b 24` to increase the size of the model, something you should always do\n",
    "* `--affix +6,-2w` to add six character prefixes to features from all namespaces and two character suffixes to features from the w namespace\n",
    "* `--spelling w` to add spelling features to the w namespace (use `--spelling _`) to add spelling features to the default namespace\n",
    "* `--ngram 3 --skips 1` to add one-skip, trigram features to all namespaces\n",
    "* `--loss_function logistic/hinge` to switch the loss function\n",
    "* How to get a human readable model using `--invert_hash`\n",
    "* Using `--early_terminate 999 --holdout_after 1401` to treat the last 200 examples as development data and turn off early stopping\n",
    "* Using namespaces and `-q` for quadratic features (there's also `--cubic` for cubic features!)\n",
    "* Using `--l2` or `--l1` to regularize the model\n",
    "* Running in neural networks mode with `--nn 10 --inpass --dropout`, and then playing around with other parameters\n",
    "\n",
    "One important thing to remember is that arguments that affect the features that `vw` use get stored in saved models. This means that if you train with `-f model` and then test with `-t -i model`, when you load the model (`-i model`), you *also* load all of the feature generators. This ensures that training and testing use a consistent feature representation, and also means you don't have to remember what arguments you used to train the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
